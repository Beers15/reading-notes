[Home](README.md)
 
# Readings: Ethics in Tech
 
## Ethics in the Workplace 

### Microsoft and the DoD <sup>1</sup>

This article discusses the morally ambiguous dealings that Microsoft has had with the DoD. In this case, Microsoft had a ~750 billion dollar contract with the DoD where they developed HoloLens augmented-reality technology. In my opinion, this is an outright ethically bankrupt move. Certain entities profiting off war is unacceptable in the first place. Having a company that publicly trades stock benefit from war is even more ridiculous. That being said, I would have to agree with the move that a coalition of Microsoft employees made when they demanded that Microsoft stop such behavior as described by the article. A lot of the software engineers that signed on to Microsoft weren't aware that someday they would be contributing to the war effort for the sake of profits instead of victory. If Microsoft truly had the war effort as a priority over profits, they could have supplied said technology for a no net gain. By simply breezing the Software Engineering Code of Ethics, one can easily see numerous ways that goes against the what the AMC believes is the standard for being an ethical engineer <sup>2</sup>. Many of the engineers involved are clearly of the same mind, but unfortunately Microsoft's shareholders and leadership are not.

-----------

## Ethics in Technology

### The ethical dilemmas of self-driving cars <sup>3</sup>

The article talks about how self driving cars are statistically much safer than their human-operated counterparts. While this is fine and dandy, the ethical dilemma arises when an autonomous vehicle AI has to decide what to do in a perilous situation. The example given in the reading was should a car steer out of the way to avoid hitting a child, but into oncoming traffic where more people will likely be critically injured, or do the opposite? Consider the similar trolly philosophical question where a person can pull a lever to save 5 people, but ensure another person's demise. What one person considers ethical is not what another would. If humans can't decide what to do, how can we decide what a machine should do? I don't have the answer for this question, but hopefully the involved software engineers can figure it out. Generally speaking though, at the end of the day it may be better to have AI act similarly to an ethical human in such scenarios rather than just doing the 'most logical' thing.

--------------

### Resources

https://www.businessinsider.com/microsoft-employees-protest-contract-us-army-hololens-2019-2 <sup>1</sup>

https://ethics.acm.org/code-of-ethics/software-engineering-code/ <sup>2</sup>

https://www.theglobeandmail.com/globe-drive/culture/technology/the-ethical-dilemmas-of-self-drivingcars/article37803470/ <sup>3</sup>